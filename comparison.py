# -*- coding: utf-8 -*-
"""comparison.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19v_UCFi83Nrx-_wQ8-lhxm7-C4jWzX3f
"""

!pip install -q jieba scikit-learn google-generativeai

import math
import jieba
import re
import json
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import google.generativeai as genai
from google.colab import userdata # Colab ç’°å¢ƒå°ˆç”¨ï¼Œç”¨æ–¼è®€å–å¯†é‘°
API_KEY = userdata.get("GOOGLE_API_KEY")
genai.configure(api_key=API_KEY)

# --- å‚³çµ±æ–¹æ³• (TF-IDF ç›¸ä¼¼åº¦) ---

def calculate_tf(word_dict, total_words):
    """è¨ˆç®—è©é » TF"""
    tf_dict = {}
    for word, count in word_dict.items():
        tf_dict[word] = count / total_words
    return tf_dict

def calculate_idf(documents, word):
    """è¨ˆç®—é€†æ–‡ä»¶é »ç‡ IDF"""
    N = len(documents)
    containing_docs = sum(1 for doc in documents if word in doc)
    return math.log(N / (1 + containing_docs)) + 1

def cosine_sim(doc1, doc2):
    """è¨ˆç®—å…©å€‹ TF-IDF å‘é‡çš„é¤˜å¼¦ç›¸ä¼¼åº¦"""
    words = set(doc1.keys()).union(set(doc2.keys()))
    v1 = [doc1.get(w, 0) for w in words]
    v2 = [doc2.get(w, 0) for w in words]
    dot = sum(a*b for a, b in zip(v1, v2))
    norm1 = math.sqrt(sum(a*a for a in v1))
    norm2 = math.sqrt(sum(a*a for a in v2))
    return dot / (norm1 * norm2) if norm1 * norm2 != 0 else 0


# --- å‚³çµ±æ–¹æ³• (åŸºæ–¼è¦å‰‡çš„åˆ†é¡å™¨) ---
#æƒ…æ„Ÿåˆ†é¡å™¨
class RuleBasedSentimentClassifier:
    def __init__(self):
        # æ­£é¢è©åº« + åŒç¾©è©
        self.positive_words = ['å¥½','æ£’','å„ªç§€','å–œæ­¡','æ¨è–¦','æ»¿æ„','é–‹å¿ƒ','å€¼å¾—','ç²¾å½©','å®Œç¾','å¥½åƒ','ç¾å‘³','é©šè‰·','æå‡',"é€²æ­¥"]
        # è² é¢è©åº« + åŒç¾©è©
        self.negative_words = ['å·®','ç³Ÿ','å¤±æœ›','è¨å­','ä¸æ¨è–¦','æµªè²»','ç„¡èŠ','çˆ›','ç³Ÿç³•','å·®å‹','ç©ºæ´','ç–²å‹','ç—›è‹¦']
        # å¦å®šè©
        self.negation_words = ['ä¸','æ²’','ç„¡','é','åˆ¥']
        # ç¨‹åº¦å‰¯è©åŠ æ¬Š
        self.intensifiers = {'éå¸¸':1.5, 'æŒº':1.1, 'è¶…':1.3}

    def classify(self, text):
        pos_score = 0
        neg_score = 0
        tokens = list(jieba.cut(text))

        i = 0
        while i < len(tokens):
            word = tokens[i]
            weight = 1
            # ç¨‹åº¦å‰¯è©
            if i > 0 and tokens[i-1] in self.intensifiers:
                weight = self.intensifiers[tokens[i-1]]
            # å¦å®šè©
            negation = False
            if i > 0 and tokens[i-1] in self.negation_words:
                negation = True
            if word in self.positive_words:
                if negation:
                    neg_score += 1 * weight
                else:
                    pos_score += 1 * weight
            elif word in self.negative_words:
                if negation:
                    pos_score += 1 * weight
                else:
                    neg_score += 1 * weight
            i += 1

        if pos_score > neg_score:
            return 'æ­£é¢'
        elif neg_score > pos_score:
            return 'è² é¢'
        else:
            return 'ä¸­æ€§'
#ä¸»é¡Œåˆ†é¡å™¨
class TopicClassifier:
    def __init__(self):
        self.topic_keywords = {
            'ç§‘æŠ€': ['AI', 'äººå·¥æ™ºæ…§', 'é›»è…¦', 'è»Ÿé«”', 'ç¨‹å¼', 'æ¼”ç®—æ³•', 'æ·±åº¦å­¸ç¿’', 'æ¨¡å‹'],
            'é‹å‹•': ['é‹å‹•', 'å¥èº«', 'è·‘æ­¥', 'æ¸¸æ³³', 'çƒé¡', 'æ¯”è³½', 'æ…¢è·‘', 'é‡è¨“', 'é«”èƒ½'],
            'ç¾é£Ÿ': ['åƒ', 'é£Ÿç‰©', 'é¤å»³', 'ç¾å‘³', 'æ–™ç†', 'çƒ¹é£ª', 'ç‰›è‚‰éºµ', 'æ¹¯é ­', 'éºµæ¢'],
            'æ—…éŠ': ['æ—…è¡Œ', 'æ™¯é»', 'é£¯åº—', 'æ©Ÿç¥¨', 'è§€å…‰', 'åº¦å‡', 'æ—…éŠ']
        }

    def classify(self, text):
        tokens = list(jieba.cut(text))
        counts = {topic: sum(1 for t in tokens if t in keywords) for topic, keywords in self.topic_keywords.items()}
        max_count = max(counts.values())
        if max_count == 0:
            return 'å…¶ä»–'
        # è¿”å›è¨ˆæ•¸æœ€é«˜çš„é‚£å€‹ä¸»é¡Œ
        return max((topic for topic, count in counts.items() if count == max_count), default='å…¶ä»–')


# --- å‚³çµ±æ–¹æ³• (çµ±è¨ˆå¼è‡ªå‹•æ‘˜è¦) ---

class StatisticalSummarizer:
    def __init__(self):
        self.stop_words = set([
            'çš„','äº†','åœ¨','æ˜¯','æˆ‘','æœ‰','å’Œ','å°±','ä¸','äºº','éƒ½','ä¸€','ä¸€å€‹','ä¸Š','ä¹Ÿ','å¾ˆ','åˆ°','èªª','è¦','å»','ä½ '
        ])

    def sentence_score(self, sentence, word_freq, position, total_sentences):
        tokens = [w for w in jieba.cut(sentence) if w not in self.stop_words and len(w.strip())>0]
        score = sum(word_freq.get(w,0) for w in tokens)
        if position == 0 or position == total_sentences-1:
            score *= 1.2
        if len(tokens) < 5:
            score *= 0.8
        if re.search(r'\d', sentence) or re.search(r'[A-Z]{2,}', sentence):
            score *= 1.1
        return score

    def summarize(self, text, ratio=0.3):
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ\n])', text)
        sentences = [s.strip() for s in sentences if len(s.strip())>0]
        words = [w for s in sentences for w in jieba.cut(s) if w not in self.stop_words and len(w.strip())>0]
        word_freq = Counter(words)
        scores = []
        total_sentences = len(sentences)
        for idx, sentence in enumerate(sentences):
            score = self.sentence_score(sentence, word_freq, idx, total_sentences)
            scores.append((sentence, score))
        top_n = max(1, int(len(sentences)*ratio))
        top_sentences = sorted(scores, key=lambda x: x[1], reverse=True)[:top_n]
        top_sentences_sorted = sorted(top_sentences, key=lambda x: sentences.index(x[0]))
        return ' '.join([s[0] for s in top_sentences_sorted])


# --- ç¾ä»£ AI æ–¹æ³• (Gemini) ---

def ai_similarity(text1, text2, api_key):
    """ä½¿ç”¨ Gemini Pro æ¨¡å‹è¨ˆç®—èªæ„ç›¸ä¼¼åº¦ (0-100 åˆ†)"""
    if not api_key: return "N/A (API Key Missing)"
    try:
        model = genai.GenerativeModel("gemini-2.5-flash-lite")
        prompt = f"""
        è«‹è©•ä¼°ä»¥ä¸‹å…©æ®µæ–‡å­—çš„èªæ„ç›¸ä¼¼åº¦ã€‚è«‹åƒ…è¼¸å‡ºä¸€å€‹æ•´æ•¸åˆ†æ•¸ï¼ˆ0-100ï¼‰ã€‚
        æ–‡å­—1ï¼š{text1}
        æ–‡å­—2ï¼š{text2}
        """
        response = model.generate_content(prompt)
        result = response.text.strip()
        score = int(''.join([c for c in result if c.isdigit()]))
        return min(100, max(0, score))
    except Exception as e:
        return f"Error: {e}"

def ai_classify_gemini(text):
    """ä½¿ç”¨ Gemini 2.5 Flash Lite é€²è¡Œå¤šç¶­åº¦æ–‡æœ¬åˆ†é¡"""
    if not API_KEY: return {"error": "N/A (API Key Missing)"}
    prompt = f"""
    è«‹é–±è®€ä»¥ä¸‹æ–‡å­—ï¼Œåˆ¤æ–·å®ƒçš„ã€Œæƒ…æ„Ÿå‚¾å‘ã€èˆ‡ã€Œä¸»é¡Œé¡åˆ¥ã€ã€‚

    æ–‡å­—ï¼š
    {text}

    è«‹ä»¥ JSON æ ¼å¼å›ç­”ï¼Œéµåç‚º sentiment (æ­£é¢/è² é¢/ä¸­æ€§) å’Œ topic (ä¸»é¡Œé¡åˆ¥)ã€‚
    è«‹åªè¼¸å‡º JSONï¼Œä¸è¦é¡å¤–æ–‡å­—ã€‚
    """
    model = genai.GenerativeModel("gemini-2.5-flash-lite")
    try:
        response = model.generate_content(prompt)
        result_text = response.text.strip()
        cleaned = re.sub(r"```json|```", "", result_text).strip()
        return json.loads(cleaned)
    except Exception as e:
        return {"error": f"AI Classification Error: {e}"}

def ai_summarize(text, max_length=150):
    """ä½¿ç”¨ Gemini-2.5-flash-lite ç”Ÿæˆè‡ªå‹•æ‘˜è¦"""
    if not API_KEY: return "N/A (API Key Missing)"
    model = genai.GenerativeModel(model_name="gemini-2.5-flash-lite")
    prompt = f"""
    è«‹å°‡ä»¥ä¸‹æ–‡ç« æ‘˜è¦æˆä¸€æ®µæµæš¢ã€é€šé †çš„ä¸­æ–‡æ–‡å­—ã€‚
    æ‘˜è¦è¦æ±‚ï¼šç›¡é‡æ§åˆ¶æ‘˜è¦é•·åº¦ç´„{max_length}å€‹å­—ã€‚
    æ–‡ç« ï¼š
    {text}
    """
    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"Error: {str(e)}"

# åŸ·è¡Œæ¯”è¼ƒæ¸¬è©¦

def run_comparison():
    print("ğŸŒŸ å‚³çµ± NLP æ–¹æ³• vs ç¾ä»£ AI æ–¹æ³•æ¯”è¼ƒ ğŸŒŸ")
    print("========================================\n")

    # 1. æ–‡æœ¬ç›¸ä¼¼åº¦æ¯”è¼ƒ
    print("--- 1. æ–‡æœ¬ç›¸ä¼¼åº¦ (Doc1: äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šä¸–ç•Œ,æ©Ÿå™¨å­¸ç¿’æ˜¯å…¶æ ¸å¿ƒæŠ€è¡“) ---")
    documents_raw = [
    "äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šä¸–ç•Œ,æ©Ÿå™¨å­¸ç¿’æ˜¯å…¶æ ¸å¿ƒæŠ€è¡“",
    "æ·±åº¦å­¸ç¿’æ¨å‹•äº†äººå·¥æ™ºæ…§çš„ç™¼å±•,ç‰¹åˆ¥æ˜¯åœ¨åœ–åƒè­˜åˆ¥é ˜åŸŸ",
    "ä»Šå¤©å¤©æ°£å¾ˆå¥½,é©åˆå‡ºå»é‹å‹•",
    "æ©Ÿå™¨å­¸ç¿’å’Œæ·±åº¦å­¸ç¿’éƒ½æ˜¯äººå·¥æ™ºæ…§çš„é‡è¦åˆ†æ”¯",
    "é‹å‹•æœ‰ç›Šå¥åº·,æ¯å¤©éƒ½æ‡‰è©²ä¿æŒé‹å‹•ç¿’æ…£"
    ]
    doc1, doc2, doc3, doc4, doc5 = documents_raw[0], documents_raw[1], documents_raw[2], documents_raw[3], documents_raw[4]

    # TF-IDF (scikit-learn)
    def tfidf_similarity(docs, target_idx=0):
        # é€²è¡Œåˆ†è©ä»¥é…åˆ TF-IDF
        tokenizer = lambda text: list(jieba.cut(text))
        vectorizer = TfidfVectorizer(tokenizer=tokenizer)
        tfidf_matrix = vectorizer.fit_transform(docs)
        cos_sim = cosine_similarity(tfidf_matrix[target_idx:target_idx+1], tfidf_matrix)
        # è¿”å›èˆ‡ Doc1 çš„ç›¸ä¼¼åº¦ï¼Œæ’é™¤èˆ‡è‡ªå·±çš„ç›¸ä¼¼åº¦ (cos_sim[0][0])
        return [f"{sim:.4f}" for sim in cos_sim[0][1:]]

    tfidf_results = tfidf_similarity(documents_raw)

    # è¼¸å‡ºæ¯”è¼ƒçµæœ
    print(f"Doc1 vs Doc2 (AI, DL): \tTF-IDF = {tfidf_results[0]}, AI = {ai_similarity(doc1, doc2, API_KEY)}")
    print(f"Doc1 vs Doc3 (AI, Weather): \tTF-IDF = {tfidf_results[1]}, AI = {ai_similarity(doc1, doc3, API_KEY)}")
    print(f"Doc1 vs Doc4 (AI, ML): \tTF-IDF = {tfidf_results[1]}, AI = {ai_similarity(doc1, doc4, API_KEY)}")
    print(f"Doc1 vs Doc5 (AI, Exercise): \tTF-IDF = {tfidf_results[1]}, AI = {ai_similarity(doc1, doc5, API_KEY)}")
    print("\n* è§€å¯Ÿï¼šAI æ¨¡å‹åœ¨æ•æ‰èªç¾©é—œè¯æ–¹é¢è¡¨ç¾æ›´ä½³ï¼Œç•¢ç«Ÿæ©Ÿå™¨å­¸ç¿’ä¸åªæ˜¯å–®ç´”çš„è©å½™è¨ˆæ•¸ã€‚")

    print("\n" + "="*40 + "\n")

    # 2. æ–‡æœ¬åˆ†é¡æ¯”è¼ƒ
    print("--- 2. æ–‡æœ¬åˆ†é¡ (æƒ…æ„Ÿèˆ‡ä¸»é¡Œ) ---")
    test_samples = [
        "é€™å®¶é¤å»³çš„ç‰›è‚‰éºµçœŸçš„å¤ªå¥½åƒäº†,æ¹¯é ­æ¿ƒéƒ,éºµæ¢Qå½ˆ,ä¸‹æ¬¡ä¸€å®šå†ä¾†!",
        "æœ€æ–°çš„AIæŠ€è¡“çªç ´è®“äººé©šè‰·,æ·±åº¦å­¸ç¿’æ¨¡å‹çš„è¡¨ç¾è¶Šä¾†è¶Šå¥½",
        "é€™éƒ¨é›»å½±åŠ‡æƒ…ç©ºæ´,æ¼”æŠ€ç³Ÿç³•,å®Œå…¨æ˜¯æµªè²»æ™‚é–“",
        "æ¯å¤©æ…¢è·‘5å…¬é‡Œ,é…åˆé©ç•¶çš„é‡è¨“,é«”èƒ½é€²æ­¥å¾ˆå¤š",
    ]
    sentiment_clf = RuleBasedSentimentClassifier()
    topic_clf = TopicClassifier()

    for i, text in enumerate(test_samples):
        print(f"\n[æ–‡æœ¬ {i+1}] {text}")

        # å‚³çµ±æ–¹æ³•
        rule_sent = sentiment_clf.classify(text)
        rule_topic = topic_clf.classify(text)
        print(f"  å‚³çµ±è¦å‰‡ï¼šæƒ…æ„Ÿ = {rule_sent}, ä¸»é¡Œ = {rule_topic}")

        # ç¾ä»£ AI æ–¹æ³•
        ai_result = ai_classify_gemini(text)
        if 'error' not in ai_result:
            print(f"  ç¾ä»£ AIï¼šæƒ…æ„Ÿ = {ai_result.get('sentiment')}, ä¸»é¡Œ = {ai_result.get('topic')}")
        else:
            print(f"  ç¾ä»£ AIï¼š{ai_result['error']}")

    print("\n* è§€å¯Ÿï¼šAI åˆ†é¡å™¨åœ¨è™•ç†è¤‡é›œèªå¢ƒæˆ–æœªç™»éŒ„è©å½™æ™‚ï¼Œé€šå¸¸æ›´ç‚ºæº–ç¢ºï¼ˆå‡è¨­å®ƒçš„è¨“ç·´æ•¸æ“šå¤ å¥½ï¼‰ã€‚")

    print("\n" + "="*40 + "\n")

    # 3. è‡ªå‹•æ‘˜è¦æ¯”è¼ƒ
    print("--- 3. è‡ªå‹•æ‘˜è¦ ---")
    article = """
äººå·¥æ™ºæ…§(AI)çš„ç™¼å±•æ­£åœ¨æ·±åˆ»æ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼ã€‚å¾æ—©ä¸Šèµ·åºŠæ™‚çš„æ™ºæ…§é¬§é˜,
åˆ°é€šå‹¤æ™‚çš„è·¯ç·šè¦åŠƒ,å†åˆ°å·¥ä½œä¸­çš„å„ç¨®è¼”åŠ©å·¥å…·,AIç„¡è™•ä¸åœ¨ã€‚
åœ¨é†«ç™‚é ˜åŸŸ,AIå”åŠ©é†«ç”Ÿé€²è¡Œç–¾ç—…è¨ºæ–·,æé«˜äº†è¨ºæ–·çš„æº–ç¢ºç‡å’Œæ•ˆç‡ã€‚é€éåˆ†æ
å¤§é‡çš„é†«ç™‚å½±åƒå’Œç—…æ­·è³‡æ–™,AIèƒ½å¤ ç™¼ç¾äººçœ¼å®¹æ˜“å¿½ç•¥çš„ç´°ç¯€,ç‚ºæ‚£è€…æä¾›æ›´å¥½
çš„æ²»ç™‚æ–¹æ¡ˆã€‚
æ•™è‚²æ–¹é¢,AIå€‹äººåŒ–å­¸ç¿’ç³»çµ±èƒ½å¤ æ ¹æ“šæ¯å€‹å­¸ç”Ÿçš„å­¸ç¿’é€²åº¦å’Œç‰¹é»,æä¾›å®¢è£½åŒ–
çš„æ•™å­¸å…§å®¹ã€‚é€™ç¨®å› ææ–½æ•™çš„æ–¹å¼,è®“å­¸ç¿’è®Šå¾—æ›´åŠ é«˜æ•ˆå’Œæœ‰è¶£ã€‚
ç„¶è€Œ,AIçš„å¿«é€Ÿç™¼å±•ä¹Ÿå¸¶ä¾†äº†ä¸€äº›æŒ‘æˆ°ã€‚é¦–å…ˆæ˜¯å°±æ¥­å•é¡Œ,è¨±å¤šå‚³çµ±å·¥ä½œå¯èƒ½æœƒ
è¢«AIå–ä»£ã€‚å…¶æ¬¡æ˜¯éš±ç§å’Œå®‰å…¨å•é¡Œ,AIç³»çµ±éœ€è¦å¤§é‡æ•¸æ“šä¾†è¨“ç·´,å¦‚ä½•ä¿è­·å€‹äºº
éš±ç§æˆç‚ºé‡è¦è­°é¡Œã€‚æœ€å¾Œæ˜¯å€«ç†å•é¡Œ,AIçš„æ±ºç­–éç¨‹å¾€å¾€ç¼ºä¹é€æ˜åº¦,å¯èƒ½æœƒç”¢
ç”Ÿåè¦‹æˆ–æ­§è¦–ã€‚
é¢å°é€™äº›æŒ‘æˆ°,æˆ‘å€‘éœ€è¦åœ¨æ¨å‹•AIç™¼å±•çš„åŒæ™‚,å»ºç«‹ç›¸æ‡‰çš„æ³•å¾‹æ³•è¦å’Œå€«ç†æº–å‰‡ã€‚
åªæœ‰é€™æ¨£,æ‰èƒ½ç¢ºä¿AIæŠ€è¡“çœŸæ­£ç‚ºäººé¡ç¦ç¥‰æœå‹™,å‰µé€ ä¸€å€‹æ›´ç¾å¥½çš„æœªä¾†ã€‚
"""
    # å‚³çµ±æ–¹æ³•
    stat_summarizer = StatisticalSummarizer()
    stat_summary = stat_summarizer.summarize(article, ratio=0.3)

    # ç¾ä»£æ–¹æ³•
    ai_summary_result = ai_summarize(article, max_length=120)

    print(f"åŸæ–‡é•·åº¦ï¼šç´„ {len(article)} å­—")
    print(f"åŸæ–‡é–‹é ­ï¼š{article.strip()[:30]}...")

    print("\n[å‚³çµ±çµ±è¨ˆæ‘˜è¦ (é—œéµå¥æ“·å–)]")
    print(stat_summary)

    print("\n[ç¾ä»£ AI æ‘˜è¦ (ç”Ÿæˆå¼)]")
    print(ai_summary_result)

    print("\n* è§€å¯Ÿï¼šå‚³çµ±æ‘˜è¦æ˜¯ã€ŒæŒ‘é¸ã€ï¼ŒAI æ‘˜è¦æ˜¯ã€Œå‰µé€ ã€ã€‚AI æ‘˜è¦èªå¥é€£è²«æ€§é€šå¸¸æ›´å¥½ï¼Œä½†å¯èƒ½ç”¢ç”ŸåŸæ–‡ä¸­æ²’æœ‰çš„å…§å®¹ï¼ˆåœ¨é€™è£¡æ©Ÿç‡è¼ƒä½ï¼‰ã€‚")


# åŸ·è¡Œæ¯”è¼ƒ
# ä¾è³´æ–¼å„²å­˜æ ¼ 1 ä¸­çš„ API_KEY è®Šæ•¸
try:
    if API_KEY and API_KEY != "N/A":
        run_comparison()
    else:
        print("ç”±æ–¼ GOOGLE_API_KEY ç¼ºå¤±æˆ–ç„¡æ•ˆï¼Œåƒ…èƒ½åŸ·è¡Œå‚³çµ± NLP éƒ¨åˆ†çš„ç¨ç«‹æ¸¬è©¦")
        # å¦‚æœéœ€è¦ï¼Œä½ å¯ä»¥åœ¨é€™è£¡é¡å¤–é‹è¡Œä¸€äº›ä¸ä¾è³´ AI çš„å‚³çµ±æ¸¬è©¦
except NameError:
     print("éŒ¯èª¤ï¼šAPI_KEY è®Šæ•¸å°šæœªåœ¨å„²å­˜æ ¼ 1 ä¸­å®šç¾©ï¼è«‹å…ˆé‹è¡Œå„²å­˜æ ¼ 1ã€‚")