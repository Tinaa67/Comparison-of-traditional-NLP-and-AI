# -*- coding: utf-8 -*-
"""traditional_methods.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MiOtqTc1SSLQxA6i5Ni_lbJOvFeWRhAC
"""

#A-1:TF-IDF文本相似度計算 #使用jieba中文分詞改善
!pip install -q jieba scikit-learn

import math
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

#測試資料
documents = [
    "人工智慧正在改變世界,機器學習是其核心技術",
    "深度學習推動了人工智慧的發展,特別是在圖像識別領域",
    "今天天氣很好,適合出去運動",
    "機器學習和深度學習都是人工智慧的重要分支",
    "運動有益健康,每天都應該保持運動習慣"
]

#A-1(1):手動計算TF-IDF
def calculate_tf(word_dict, total_words):
    """計算詞頻 TF"""
    tf_dict = {}
    for word, count in word_dict.items():
        tf_dict[word] = count / total_words
    return tf_dict
def calculate_idf(documents, word):
    """計算逆文件頻率 IDF"""
    N = len(documents)
    containing_docs = sum(1 for doc in documents if word in doc)
    return math.log(N / (1 + containing_docs)) + 1  # 加1避免除以0

#進行中文斷詞
tokenized_docs = [list(jieba.cut(doc)) for doc in documents]

#計算每個文件的詞頻
tf_list = []
for tokens in tokenized_docs:
    word_dict = {}
    for word in tokens:
        word_dict[word] = word_dict.get(word, 0) + 1
    tf_list.append(calculate_tf(word_dict, len(tokens)))

#計算所有詞彙的IDF
vocab = set(word for doc in tokenized_docs for word in doc)
idf_dict = {word: calculate_idf(tokenized_docs, word) for word in vocab}

#計算每個文件的TF-IDF向量
tfidf_docs = []
for tf in tf_list:
    tfidf_docs.append({word: tf[word] * idf_dict[word] for word in tf})

#計算餘弦相似度
def cosine_sim(doc1, doc2):
    words = set(doc1.keys()).union(set(doc2.keys()))
    v1 = [doc1.get(w, 0) for w in words]
    v2 = [doc2.get(w, 0) for w in words]
    dot = sum(a*b for a, b in zip(v1, v2))
    norm1 = math.sqrt(sum(a*a for a in v1))
    norm2 = math.sqrt(sum(a*a for a in v2))
    return dot / (norm1 * norm2) if norm1 * norm2 != 0 else 0

print("=== 手動計算 TF-IDF 相似度 ===")
print("Doc1 vs Doc2:", cosine_sim(tfidf_docs[0], tfidf_docs[1]))
print("Doc1 vs Doc3:", cosine_sim(tfidf_docs[0], tfidf_docs[2]))
print("Doc1 vs Doc4:", cosine_sim(tfidf_docs[0], tfidf_docs[3]))
print("Doc1 vs Doc5:", cosine_sim(tfidf_docs[0], tfidf_docs[4]))


#A-1(2):使用scikit-learn計算TF-IDF與相似度
def jieba_tokenizer(text):
    return list(jieba.cut(text))

vectorizer = TfidfVectorizer(tokenizer=jieba_tokenizer)
tfidf_matrix = vectorizer.fit_transform(documents)
cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)

print("\n=== scikit-learn 相似度 ===")
for i, sim in enumerate(cos_sim[0]):
    print(f"Doc1 vs Doc{i+1}: {sim:.4f}")

#A-2:基於規則的文本分類
!pip install -q jieba
import jieba

#情感分類器
class RuleBasedSentimentClassifier:
    def __init__(self):
        # 正面詞庫 + 同義詞
        self.positive_words = ['好','棒','優秀','喜歡','推薦','滿意','開心','值得','精彩','完美','好吃','美味','驚艷','提升',"進步"]
        # 負面詞庫 + 同義詞
        self.negative_words = ['差','糟','失望','討厭','不推薦','浪費','無聊','爛','糟糕','差勁','空洞','疲勞','痛苦']
        # 否定詞
        self.negation_words = ['不','沒','無','非','別']
        # 程度副詞加權
        self.intensifiers = {'非常':1.5, '挺':1.1, '超':1.3}

    def classify(self, text):
        pos_score = 0
        neg_score = 0
        tokens = list(jieba.cut(text))

        i = 0
        while i < len(tokens):
            word = tokens[i]
            weight = 1
            # 程度副詞
            if i > 0 and tokens[i-1] in self.intensifiers:
                weight = self.intensifiers[tokens[i-1]]
            # 否定詞
            negation = False
            if i > 0 and tokens[i-1] in self.negation_words:
                negation = True
            if word in self.positive_words:
                if negation:
                    neg_score += 1 * weight
                else:
                    pos_score += 1 * weight
            elif word in self.negative_words:
                if negation:
                    pos_score += 1 * weight
                else:
                    neg_score += 1 * weight
            i += 1

        if pos_score > neg_score:
            return '正面'
        elif neg_score > pos_score:
            return '負面'
        else:
            return '中性'

#主題分類器
class TopicClassifier:
    def __init__(self):
        self.topic_keywords = {
            '科技': ['AI', '人工智慧', '電腦', '軟體', '程式', '演算法', '深度學習', '模型'],
            '運動': ['運動', '健身', '跑步', '游泳', '球類', '比賽', '慢跑', '重訓', '體能'],
            '美食': ['吃', '食物', '餐廳', '美味', '料理', '烹飪', '牛肉麵', '湯頭', '麵條'],
            '旅遊': ['旅行', '景點', '飯店', '機票', '觀光', '度假', '旅遊']
        }

    def classify(self, text):
        tokens = list(jieba.cut(text))
        counts = {}
        for topic, keywords in self.topic_keywords.items():
            counts[topic] = sum(1 for t in tokens if t in keywords)
        max_count = max(counts.values())
        if max_count == 0:
            return '其他'
        for topic, count in counts.items():
            if count == max_count:
                return topic
#測試資料
test_texts = [
    "這家餐廳的牛肉麵真的太好吃了,湯頭濃郁,麵條Q彈,下次一定再來!",
    "最新的AI技術突破讓人驚艷,深度學習模型的表現越來越好",
    "這部電影劇情空洞,演技糟糕,完全是浪費時間",
    "每天慢跑5公里,配合適當的重訓,體能進步很多"
]

#建立分類器
sentiment_classifier = RuleBasedSentimentClassifier()
topic_classifier = TopicClassifier()

#執行分類
for text in test_texts:
    sentiment = sentiment_classifier.classify(text)
    topic = topic_classifier.classify(text)
    print(f"文本: {text}\n情感: {sentiment}, 主題: {topic}\n")

#A-3:統計式自動摘要
import jieba
import re
from collections import Counter

class StatisticalSummarizer:
    def __init__(self):
        #中文停用詞
        self.stop_words = set([
            '的','了','在','是','我','有','和','就','不','人','都','一','一個','上','也','很','到','說','要','去','你'
        ])
    def sentence_score(self, sentence, word_freq, position, total_sentences):
        """
        計算句子重要性分數
        考慮因素:
        1. 包含高頻詞數量
        2. 句子位置(首尾句加權)
        3. 句子長度(太短或太長扣分)
        4. 是否包含數字或專有名詞
        """
        tokens = [w for w in jieba.cut(sentence) if w not in self.stop_words and len(w.strip())>0]

        #高頻詞數量
        score = sum(word_freq.get(w,0) for w in tokens)
        #句子位置加權
        if position == 0 or position == total_sentences-1:
            score *= 1.2  #首尾句加權
        #句子長度加權
        if len(tokens) < 5:
            score *= 0.8  #太短扣分
        elif len(tokens) > 50:
            score *= 0.9  #太長略扣分
        #是否包含數字或專有名詞
        if re.search(r'\d', sentence):
            score *= 1.1  #包含數字加分
        #簡單判斷專有名詞:假設連續兩個以上中文大寫字母或括號內字母
        if re.search(r'[A-Z]{2,}', sentence) or re.search(r'\([A-Z]+\)', sentence):
            score *= 1.1
        return score

    def summarize(self, text, ratio=0.3):
        #分句(中文標點)
        sentences = re.split(r'(?<=[。！？\n])', text)
        sentences = [s.strip() for s in sentences if len(s.strip())>0]
        #分詞並計算詞頻
        words = [w for s in sentences for w in jieba.cut(s) if w not in self.stop_words and len(w.strip())>0]
        word_freq = Counter(words)
        #計算句子分數
        scores = []
        total_sentences = len(sentences)
        for idx, sentence in enumerate(sentences):
            score = self.sentence_score(sentence, word_freq, idx, total_sentences)
            scores.append((sentence, score))
        #選擇最高分句子(依比例選取)
        top_n = max(1, int(len(sentences)*ratio))
        top_sentences = sorted(scores, key=lambda x: x[1], reverse=True)[:top_n]
        #按原文順序排列
        top_sentences_sorted = sorted(top_sentences, key=lambda x: sentences.index(x[0]))
        #返回摘要
        summary = ' '.join([s[0] for s in top_sentences_sorted])
        return summary

#測試
article = """
人工智慧(AI)的發展正在深刻改變我們的生活方式。從早上起床時的智慧鬧鐘,
到通勤時的路線規劃,再到工作中的各種輔助工具,AI無處不在。
在醫療領域,AI協助醫生進行疾病診斷,提高了診斷的準確率和效率。透過分析
大量的醫療影像和病歷資料,AI能夠發現人眼容易忽略的細節,為患者提供更好
的治療方案。
教育方面,AI個人化學習系統能夠根據每個學生的學習進度和特點,提供客製化
的教學內容。這種因材施教的方式,讓學習變得更加高效和有趣。
然而,AI的快速發展也帶來了一些挑戰。首先是就業問題,許多傳統工作可能會
被AI取代。其次是隱私和安全問題,AI系統需要大量數據來訓練,如何保護個人
隱私成為重要議題。最後是倫理問題,AI的決策過程往往缺乏透明度,可能會產
生偏見或歧視。
面對這些挑戰,我們需要在推動AI發展的同時,建立相應的法律法規和倫理準則。
只有這樣,才能確保AI技術真正為人類福祉服務,創造一個更美好的未來。
"""
summarizer = StatisticalSummarizer()
summary = summarizer.summarize(article, ratio=0.3)
print("=== 自動摘要結果 ===")
print(summary)